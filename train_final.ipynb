{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.PreviousStateTransformer import PreviousStateTransformer\n",
    "from transformers.LastStateTransformer import LastStateTransformer\n",
    "from transformers.AggregateTransformer import AggregateTransformer\n",
    "from transformers.IndexBasedTransformer import IndexBasedTransformer\n",
    "from transformers.ComplexIndexBasedTransformer import ComplexIndexBasedTransformer\n",
    "from transformers.ComplexIndexNgramTransformer import ComplexIndexNgramTransformer\n",
    "from transformers.StaticTransformer import StaticTransformer\n",
    "import pm4py\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy.spatial import distance\n",
    "warnings.filterwarnings(action='ignore')\n",
    "dir_home = os.getcwd()\n",
    "\n",
    "dir_ref = dir_home +\"/ref\"\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def mydist(x, y):\n",
    "    return np.sum(x*y) # for minimizing problem\n",
    "\n",
    "def nearest_neighbors(values, all_values, nbr_neighbors, algorithm, metric, metric_params:None):\n",
    "    nn = NearestNeighbors(n_neighbors = nbr_neighbors, algorithm=algorithm, metric = metric, metric_params = metric_params).fit(all_values)  # algo: 'auto', 'ball_tree', 'kd_tree', 'brute'\n",
    "    dists, idxs = nn.kneighbors(values)\n",
    "\n",
    "    return dists, idxs\n",
    "\n",
    "\n",
    "def nearest_neighbors2(values, all_values, nbr_neighbors):\n",
    "    nn = NearestNeighbors(n_neighbors = nbr_neighbors, algorithm='ball_tree', metric = 'pyfunc', metric_params={\"func\":mydist}).fit(all_values)  # algo: 'auto', 'ball_tree', 'kd_tree', 'brute'\n",
    "    dists, idxs = nn.kneighbors(values)\n",
    "    return dists, idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The result in Table 4 can be obtained by running below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n",
      "aggregate\n",
      "index\n",
      "laststate\n",
      "aggngram\n"
     ]
    }
   ],
   "source": [
    "# SEPSIS DIST - general (k rate)\n",
    "name = 'sepsis'\n",
    "split = 0.5\n",
    "rate = 0.3\n",
    "encoding = ['bool', 'aggregate',  'index',  'laststate', 'aggngram']\n",
    "metric = [ 'cosine', 'euclidean', 'manhattan']\n",
    "\n",
    "m=7\n",
    "result = pd.DataFrame(columns=['data', 'encoding_method' , 'distance_metric', 'precision1', 'precision2',\n",
    "                               'mean', 'std', 'min', 'max', 'sim', 'time'])\n",
    "def minmax(x):\n",
    "    return (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "for e in encoding:\n",
    "    print(e)\n",
    "    data = pd.read_csv(dir_home + '/data_trans/' + name + \"_\" + e + \"_\" + str(m)  +\".csv\")\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    train = data[~data['Case ID'].astype(str).str.contains('\\_', na=False)].reset_index(drop=True)\n",
    "    test = data[data['Case ID'].astype(str).str.contains('\\_', na=False)].reset_index(drop=True)\n",
    "\n",
    "    train.index = train['Case ID']\n",
    "    train = train.drop( 'Case ID', axis=1)\n",
    "    test.index = test['Case ID']\n",
    "    test = test.drop( 'Case ID', axis=1)\n",
    "    \n",
    "    if e == \"ngram\" or \"complexngram\" or \"aggngram\":\n",
    "        train_trace = train.filter(regex='\\|')\n",
    "        test_trace = test.filter(regex='\\|')\n",
    "    else:\n",
    "        train_trace = train.filter(regex='Activity_')\n",
    "        test_trace = test.filter(regex='Activity_')    \n",
    "    \n",
    "    train_attr = train.drop(train_trace.columns , axis =1)\n",
    "    test_attr = test.drop(test_trace.columns, axis = 1)\n",
    "    \n",
    "\n",
    "    w_a = split*np.repeat(1, len(train_trace.columns))/len(train_trace.columns)\n",
    "    w_a = w_a.tolist()\n",
    "\n",
    "    w_b = pd.DataFrame([str.split(c, \"_\")[0] for c in train_attr.columns], columns=['key'] )\n",
    "    w_b['weight'] = 1\n",
    "    w_b['weight2'] = w_b.groupby('key')['weight'].cumsum()\n",
    "    w_b['max'] = w_b.groupby('key')['weight2'].transform(max)\n",
    "    w_b['weight3'] = w_b['weight']/w_b['max']\n",
    "\n",
    "    w_b = w_b['weight3'].tolist()\n",
    "    w_b = [(1-split)*w/sum(w_b) for w in w_b]\n",
    "    \n",
    "    loc_a = [ train.columns.tolist().index(ttt)  for ttt in train_trace.columns]\n",
    "    loc_b = [ train.columns.tolist().index(ttt)  for ttt in train_attr.columns]\n",
    "    customized_weights = np.repeat(0.0, len(train.columns))\n",
    "    customized_weights[loc_a] = w_a\n",
    "    customized_weights[loc_b] = w_b\n",
    "    \n",
    "    train = train.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    test = test.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    \n",
    "    \n",
    "    encoded_train = train.values.tolist()\n",
    "    encoded_test = test.values.tolist()\n",
    "    \n",
    "    encoded_train2 = train_trace.values.tolist()\n",
    "    encoded_test2 = test_trace.values.tolist()\n",
    "\n",
    "    encoded_test_attr = test_attr.values.tolist()\n",
    "    for d in metric:\n",
    "        time_start_align = time.time()\n",
    "        \n",
    "        dists, idxs = nearest_neighbors(np.array(encoded_test), \n",
    "                                    np.array(encoded_train), \n",
    "                                    nbr_neighbors = int(np.floor(len(train)*rate)), \n",
    "                                    algorithm = 'auto',\n",
    "                                    metric = d,\n",
    "                                    metric_params = None) # 20 , len(sn_encoded_train) \n",
    "        \n",
    "\n",
    "        predict = list()\n",
    "        dist_mean = list()\n",
    "        dist_std = list()\n",
    "        dist_min = list()\n",
    "        dist_max = list()\n",
    "        sim = list()\n",
    "        for i in range(0, len(test_trace)):\n",
    "            predict.append(  train.index[idxs[i]] )\n",
    "            dist_mean.append(  np.mean(dists[i]) )\n",
    "            dist_std.append(  np.std(dists[i]) )\n",
    "            dist_min.append(  np.min(dists[i]) )\n",
    "            dist_max.append(  np.max(dists[i]) )\n",
    "            sim.append( np.mean( 1/(1+dists[i])) )\n",
    "            \n",
    "            \n",
    "        time_finish_align = time.time()\n",
    "\n",
    "\n",
    "        with open(\"ref_cocomot\" , \"rb\") as fp:\n",
    "            ref_predict = pickle.load(fp, encoding='utf-8') \n",
    "        with open(\"ref_leven\" , \"rb\") as fp:\n",
    "            ref_predict2 = pickle.load(fp, encoding='utf-8') \n",
    "        \n",
    "        recall_sum = 0\n",
    "        precision_sum = 0\n",
    "        acc_sum1 = 0 \n",
    "        acc_sum2 = 0 \n",
    "        for l in range(0,30):  # change\n",
    "            i1 = ref_predict[l]\n",
    "            acc_1 = sum([1 for j in i1 if j[0] in predict[l]])/len(i1)\n",
    "            i2 = ref_predict2[l]\n",
    "            acc_2 = sum([1 for j in i2 if j[0] in predict[l]])/len(i2)\n",
    "            \n",
    "            acc_sum1 = acc_sum1 + acc_1\n",
    "            acc_sum2 = acc_sum2 + acc_2\n",
    "        acc1 = acc_sum1/30\n",
    "        acc2 = acc_sum2/30\n",
    "\n",
    "        result.loc[len(result)+1] = [name, e, d, acc1, acc2, np.mean(dist_mean), np.mean(dist_std),\n",
    "                                np.mean(dist_min), np.mean(dist_max), np.mean(sim),\n",
    "                                (time_finish_align - time_start_align) ]\n",
    "        \n",
    "result.to_csv(dir_home+\"/result/result_dist30_\" + name + \"_\" + str(rate) +\"_\"  + str(split) + \".csv\", index= False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The result in Table 6 can be obtained by running below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n",
      "0.6062214432521722\n",
      "0.6065076527028136\n",
      "0.6062935329900856\n",
      "aggregate\n",
      "0.6079201295932517\n",
      "0.6081753486347631\n",
      "0.6082110928369853\n",
      "index\n",
      "0.6196312613847257\n",
      "0.6195982285768463\n",
      "0.6195989747372875\n",
      "laststate\n",
      "0.6020542505033757\n",
      "0.5995266624451614\n",
      "0.5995264535077536\n",
      "aggngram\n",
      "0.606774927940437\n",
      "0.6065641960112671\n",
      "0.6067143947371026\n"
     ]
    }
   ],
   "source": [
    "# SEPSIS DIST - general (k size)\n",
    "name = 'sepsis'\n",
    "split = 0.5\n",
    "size = 10\n",
    "encoding = ['bool', 'aggregate',  'index',  'laststate', 'aggngram']\n",
    "metric = [ 'cosine', 'euclidean', 'manhattan']\n",
    "\n",
    "m=7\n",
    "result = pd.DataFrame(columns=['data', 'encoding_method' , 'distance_metric', 'precision1', 'precision2',\n",
    "                               'mean', 'std', 'min', 'max', 'sim1', 'sim2', 'time'])\n",
    "def minmax(x):\n",
    "    return (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "for e in encoding:\n",
    "    print(e)\n",
    "    data = pd.read_csv(dir_home + '/data_trans/' + name + \"_\" + e + \"_\" + str(m)  +\".csv\")\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    max_l = data.max(axis=0).values \n",
    "    ll = 0 \n",
    "    col_loc =list()\n",
    "    col_norm = list()\n",
    "    for ml in max_l:\n",
    "        if type(ml)==str:\n",
    "            pass\n",
    "        else:\n",
    "            if ml > 1:\n",
    "                col_loc.append(ll)\n",
    "                col_norm.append(ml)\n",
    "        ll = ll+1     \n",
    "        \n",
    "    for ll in range(len(col_loc)):\n",
    "        data[data.columns[col_loc[ll]] ]  = data[data.columns[col_loc[ll]] ]/col_norm[ll]    \n",
    "    \n",
    "    train = data[~data['Case ID'].astype(str).str.contains('\\_', na=False)].reset_index(drop=True)\n",
    "    test = data[data['Case ID'].astype(str).str.contains('\\_', na=False)].reset_index(drop=True)\n",
    "\n",
    "    train.index = train['Case ID']\n",
    "    train = train.drop( 'Case ID', axis=1)\n",
    "    test.index = test['Case ID']\n",
    "    test = test.drop( 'Case ID', axis=1)\n",
    "    \n",
    "    if e == \"ngram\" or \"complexngram\":\n",
    "        train_trace = train.filter(regex='\\|')\n",
    "        test_trace = test.filter(regex='\\|')\n",
    "    else:\n",
    "        train_trace = train.filter(regex='Activity_')\n",
    "        test_trace = test.filter(regex='Activity_')    \n",
    "    \n",
    "    train_attr = train.drop(train_trace.columns , axis =1)\n",
    "    test_attr = test.drop(test_trace.columns, axis = 1)\n",
    "    \n",
    "\n",
    "    w_a = split*np.repeat(1, len(train_trace.columns))/len(train_trace.columns)\n",
    "    w_a = w_a.tolist()\n",
    "\n",
    "    w_b = pd.DataFrame([str.split(c, \"_\")[0] for c in train_attr.columns], columns=['key'] )\n",
    "    w_b['weight'] = 1\n",
    "    w_b['weight2'] = w_b.groupby('key')['weight'].cumsum()\n",
    "    w_b['max'] = w_b.groupby('key')['weight2'].transform(max)\n",
    "    w_b['weight3'] = w_b['weight']/w_b['max']\n",
    "\n",
    "    w_b = w_b['weight3'].tolist()\n",
    "    w_b = [(1-split)*w/sum(w_b) for w in w_b]\n",
    "    \n",
    "    \n",
    "    loc_a = [ train.columns.tolist().index(ttt)  for ttt in train_trace.columns]\n",
    "    loc_b = [ train.columns.tolist().index(ttt)  for ttt in train_attr.columns]\n",
    "    customized_weights = np.repeat(0.0, len(train.columns))\n",
    "    customized_weights[loc_a] = w_a\n",
    "    customized_weights[loc_b] = w_b\n",
    "    \n",
    "    train = train.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    test = test.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    \n",
    "    \n",
    "    encoded_train = train.values.tolist()\n",
    "    encoded_test = test.values.tolist()\n",
    "    \n",
    "    encoded_train2 = train_trace.values.tolist()\n",
    "    encoded_test2 = test_trace.values.tolist()\n",
    "\n",
    "    encoded_test_attr = test_attr.values.tolist()\n",
    "    for d in metric:\n",
    "        time_start_align = time.time()\n",
    "        \n",
    "        dists, idxs = nearest_neighbors(np.array(encoded_test), \n",
    "                                    np.array(encoded_train), \n",
    "                                    nbr_neighbors = size, \n",
    "                                    algorithm = 'auto',\n",
    "                                    metric = d,\n",
    "                                    metric_params = None) # 20 , len(sn_encoded_train) \n",
    "        \n",
    "        \n",
    "        predict = list()\n",
    "        dist_mean = list()\n",
    "        dist_std = list()\n",
    "        dist_min = list()\n",
    "        dist_max = list()\n",
    "        sim= list()\n",
    "        for i in range(0, len(test_trace)):\n",
    "            predict.append(  train.index[idxs[i]] )\n",
    "            dist_mean.append(  np.mean(dists[i]) )\n",
    "            dist_std.append(  np.std(dists[i]) )\n",
    "            dist_min.append(  np.min(dists[i]) )\n",
    "            dist_max.append(  np.max(dists[i]) )\n",
    "            sim.append( np.mean( 1-dists[i]) )\n",
    "            \n",
    "            \n",
    "        time_finish_align = time.time()\n",
    "\n",
    "        with open(\"ref_cocomot\" , \"rb\") as fp:\n",
    "            ref_predict = pickle.load(fp, encoding='utf-8') \n",
    "        with open(\"ref_leven\" , \"rb\") as fp:\n",
    "            ref_predict2 = pickle.load(fp, encoding='utf-8') \n",
    "        \n",
    "        recall_sum = 0\n",
    "        precision_sum = 0\n",
    "        acc_sum1 = 0 \n",
    "        acc_sum2 = 0 \n",
    "        dist1 = 0\n",
    "        dist2 = 0\n",
    "        sim1_sum = 0\n",
    "        sim2_sum = 0\n",
    "        for l in range(0,30):  # change\n",
    "            i1 = ref_predict[l]\n",
    "            acc_1 = sum([1 for j in i1 if j[0] in predict[l]])/len(i1)\n",
    "            i2 = ref_predict2[l]\n",
    "            acc_2 = sum([1 for j in i2 if j[0] in predict[l]])/len(i2)\n",
    "            \n",
    "            opt1 = train.loc[train.index.isin([rp1[0] for rp1 in i1.tolist()])]\n",
    "            opt2 = train.loc[train.index.isin([rp1[0] for rp1 in i2.tolist()])]\n",
    "            \n",
    "            apa = train.loc[train.index.isin([rp1[0] for rp1 in predict[l]])]\n",
    "            \n",
    "            sim1 = 0\n",
    "            sim2 = 0\n",
    "            \n",
    "            for apa_i in range(len(apa)):\n",
    "                sim1_i = sum( [1-distance.euclidean(apa.iloc[apa_i] , opt1.iloc[opt1_i]) for opt1_i in range(len(opt1)) ] )/len(opt1)\n",
    "                sim2_i = sum( [1-distance.euclidean(apa.iloc[apa_i] , opt2.iloc[opt2_i]) for opt2_i in range(len(opt2)) ] )/len(opt2)\n",
    "                \n",
    "                sim1 = sim1+ sim1_i\n",
    "                sim2 = sim2+ sim2_i\n",
    "            \n",
    "            \n",
    "            acc_sum1 = acc_sum1 + acc_1\n",
    "            acc_sum2 = acc_sum2 + acc_2\n",
    "            \n",
    "            sim1_sum = sim1_sum + sim1/(len(apa)*len(opt1))\n",
    "            sim2_sum = sim2_sum + sim2/(len(apa)*len(opt2))\n",
    "        acc1 = acc_sum1/30\n",
    "        acc2 = acc_sum2/30\n",
    "        \n",
    "        sim1_out = sim1_sum/30\n",
    "        sim2_out = sim2_sum/30\n",
    "        print(sim1_out)\n",
    "        result.loc[len(result)+1] = [name, e, d, acc1, acc2, np.mean(dist_mean), np.mean(dist_std),\n",
    "                                np.mean(dist_min), np.mean(dist_max), np.mean(sim1_out), np.mean(sim2_out),\n",
    "                                (time_finish_align - time_start_align) ]\n",
    "        \n",
    "result.to_csv(dir_home+\"/result/result_sim_\" + name + \"_\" + str(size) +\"_\"  + str(split) + \".csv\", index= False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The result in Table 3 can be obtained by running below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n",
      "0.05517241379310345\n",
      "0.1129973474801061\n",
      "0.1129973474801061\n",
      "aggregate\n",
      "0.07374005305039788\n",
      "0.047214854111405836\n",
      "0.04880636604774536\n",
      "index\n",
      "0.3310344827586207\n",
      "0.329973474801061\n",
      "0.329973474801061\n",
      "laststate\n",
      "0.09124668435013263\n",
      "0.0986737400530504\n",
      "0.0986737400530504\n",
      "aggngram\n",
      "0.5517241379310345\n",
      "0.5575596816976127\n",
      "0.5692307692307692\n"
     ]
    }
   ],
   "source": [
    "# Road - general (k rate)\n",
    "name = 'road'\n",
    "rate = 0.02\n",
    "split = 0.5\n",
    "encoding = ['bool', 'aggregate',  'index',  'laststate', 'aggngram']\n",
    "metric = [ 'cosine', 'euclidean', 'manhattan']\n",
    "m=1\n",
    "result = pd.DataFrame(columns=['data', 'encoding_method' , 'distance_metric', 'precision', \n",
    "                               'mean', 'std', 'min', 'max', 'sim', 'time'])\n",
    "def minmax(x):\n",
    "    return (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "for e in encoding:\n",
    "    print(e)\n",
    "    data = pd.read_csv(dir_home + '/data_trans/' + name + \"_\" + e + \"_\" + str(m)  +\".csv\")\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "\n",
    "    train = data[~data['Case ID'].astype(str).str.contains('test', na=False)].reset_index(drop=True)\n",
    "    test = data[data['Case ID'].astype(str).str.contains('test', na=False)].reset_index(drop=True)\n",
    "\n",
    "    train.index = train['Case ID']\n",
    "    train = train.drop( 'Case ID', axis=1)\n",
    "    test.index = test['Case ID']\n",
    "    test = test.drop( 'Case ID', axis=1)\n",
    "    \n",
    "    if e == \"ngram\":\n",
    "        train_trace = train.filter(regex='\\|')\n",
    "        test_trace = test.filter(regex='\\|')\n",
    "    else:\n",
    "        train_trace = train.filter(regex='Activity_')\n",
    "        test_trace = test.filter(regex='Activity_')    \n",
    "    \n",
    "    train_attr = train.drop(train_trace.columns , axis =1)\n",
    "    test_attr = test.drop(test_trace.columns, axis = 1)\n",
    "\n",
    "    w_a = split*np.repeat(1, len(train_trace.columns))/len(train_trace.columns)\n",
    "    w_a = w_a.tolist()\n",
    "\n",
    "    w_b = pd.DataFrame([str.split(c, \"_\")[0] for c in train_attr.columns], columns=['key'] )\n",
    "    w_b['weight'] = 1\n",
    "    w_b['weight2'] = w_b.groupby('key')['weight'].cumsum()\n",
    "    w_b['max'] = w_b.groupby('key')['weight2'].transform(max)\n",
    "    w_b['weight3'] = w_b['weight']/w_b['max']\n",
    "\n",
    "    w_b = w_b['weight3'].tolist()\n",
    "    w_b = [(1-split)*w/sum(w_b) for w in w_b]\n",
    "    \n",
    "    \n",
    "    loc_a = [ train.columns.tolist().index(ttt)  for ttt in train_trace.columns]\n",
    "    loc_b = [ train.columns.tolist().index(ttt)  for ttt in train_attr.columns]\n",
    "    customized_weights = np.repeat(0.0, len(train.columns))\n",
    "    customized_weights[loc_a] = w_a\n",
    "    customized_weights[loc_b] = w_b\n",
    "\n",
    "    train = train.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    test = test.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    \n",
    "    encoded_train = train.values.tolist()\n",
    "    encoded_test = test.values.tolist()\n",
    "\n",
    "    encoded_test_attr = test_attr.values.tolist()\n",
    "    for d in metric:\n",
    "        time_start_align = time.time()\n",
    "        \n",
    "        \n",
    "        dists, idxs = nearest_neighbors(np.array(encoded_test), \n",
    "                                    np.array(encoded_train), \n",
    "                                    nbr_neighbors = int(np.floor(len(train)*rate)), \n",
    "                                    algorithm = 'auto',\n",
    "                                    metric = d,\n",
    "                                    metric_params = None) # 20 , len(sn_encoded_train) \n",
    "        \n",
    "        predict = list()\n",
    "        dist_mean = list()\n",
    "        dist_std = list()\n",
    "        dist_min = list()\n",
    "        dist_max = list()\n",
    "        sim = list()\n",
    "        for i in range(0, len(test_trace)):        \n",
    "            predict.append(  train.index[idxs[i]] )\n",
    "            dist_mean.append(  np.mean(dists[i]) )\n",
    "            dist_std.append(  np.std(dists[i]) )\n",
    "            dist_min.append(  np.min(dists[i]) )\n",
    "            dist_max.append(  np.max(dists[i]) )\n",
    "            sim.append( np.mean( 1/(1+dists[i])) )\n",
    "\n",
    "        time_finish_align = time.time()\n",
    "\n",
    "        with open(\"label_\"+name , \"rb\") as fp:\n",
    "            ref_predict = pickle.load(fp, encoding='utf-8') \n",
    "        recall_sum = 0\n",
    "        precision_sum = 0\n",
    "        acc_sum1 = 0 \n",
    "        acc_sum2 = 0 \n",
    "        count=0\n",
    "        for l in range(0, len(test_trace)):  # change\n",
    "            i = ref_predict[l]\n",
    "            if len(i) >0:\n",
    "                acc_1 = (i[0] in predict[l])\n",
    "                acc_sum1 = acc_sum1 + acc_1\n",
    "                count += 1\n",
    "            \n",
    "        acc1 = acc_sum1/count\n",
    "        print(acc1)\n",
    "        result.loc[len(result)+1] = [name, e, d, acc1, np.mean(dist_mean), np.mean(dist_std),\n",
    "                                np.mean(dist_min), np.mean(dist_max), np.mean(sim),\n",
    "                                (time_finish_align - time_start_align) ]\n",
    "result.to_csv(dir_home+\"/result/result_dist5_\" + name + \"_\" + str(rate) +\"_\"  + str(split) + \".csv\", index= False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The result in Table 5 can be obtained by running below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n",
      "0.9772875026236928\n",
      "0.9773446210708625\n",
      "0.9774802675376904\n",
      "aggregate\n",
      "0.9730420401232429\n",
      "0.9596797483969746\n",
      "0.9476288006232388\n",
      "index\n",
      "0.9901732410980985\n",
      "0.9902191191870047\n",
      "0.9811627326996627\n",
      "laststate\n",
      "0.9841393163844594\n",
      "0.9840657445049604\n",
      "0.9840617713187639\n",
      "aggngram\n",
      "0.9953259401759348\n",
      "0.9953394155491504\n",
      "0.9953428445193031\n"
     ]
    }
   ],
   "source": [
    "# Road - general (k size)\n",
    "name = 'road'\n",
    "size = 10\n",
    "split = 0.5\n",
    "encoding = ['bool', 'aggregate',  'index',  'laststate', 'aggngram']\n",
    "metric = [ 'cosine', 'euclidean', 'manhattan']\n",
    "\n",
    "m=1\n",
    "result = pd.DataFrame(columns=['data', 'encoding_method' , 'distance_metric', 'precision', \n",
    "                               'mean', 'std', 'min', 'max', 'sim1' , 'time'])\n",
    "def minmax(x):\n",
    "    return (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "for e in encoding:\n",
    "    print(e)\n",
    "    data = pd.read_csv(dir_home + '/data_trans/' + name + \"_\" + e + \"_\" + str(m)  +\".csv\")\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    max_l = data.max(axis=0).values \n",
    "    ll = 0 \n",
    "    col_loc =list()\n",
    "    col_norm = list()\n",
    "    for ml in max_l:\n",
    "        if type(ml)==str:\n",
    "            pass\n",
    "        else:\n",
    "            if ml > 1:\n",
    "                col_loc.append(ll)\n",
    "                col_norm.append(ml)\n",
    "        ll = ll+1     \n",
    "        \n",
    "    for ll in range(len(col_loc)):\n",
    "        data[data.columns[col_loc[ll]] ]  = data[data.columns[col_loc[ll]] ]/col_norm[ll]    \n",
    "    train = data[~data['Case ID'].astype(str).str.contains('test', na=False)].reset_index(drop=True)\n",
    "    test = data[data['Case ID'].astype(str).str.contains('test', na=False)].reset_index(drop=True)\n",
    "\n",
    "    train.index = train['Case ID']\n",
    "    train = train.drop( 'Case ID', axis=1)\n",
    "    test.index = test['Case ID']\n",
    "    test = test.drop( 'Case ID', axis=1)\n",
    "    \n",
    "    if e == \"ngram\":\n",
    "        train_trace = train.filter(regex='\\|')\n",
    "        test_trace = test.filter(regex='\\|')\n",
    "    else:\n",
    "        train_trace = train.filter(regex='Activity_')\n",
    "        test_trace = test.filter(regex='Activity_')    \n",
    "    \n",
    "    train_attr = train.drop(train_trace.columns , axis =1)\n",
    "    test_attr = test.drop(test_trace.columns, axis = 1)\n",
    "\n",
    "    w_a = split*np.repeat(1, len(train_trace.columns))/len(train_trace.columns)\n",
    "    w_a = w_a.tolist()\n",
    "\n",
    "    w_b = pd.DataFrame([str.split(c, \"_\")[0] for c in train_attr.columns], columns=['key'] )\n",
    "    w_b['weight'] = 1\n",
    "    w_b['weight2'] = w_b.groupby('key')['weight'].cumsum()\n",
    "    w_b['max'] = w_b.groupby('key')['weight2'].transform(max)\n",
    "    w_b['weight3'] = w_b['weight']/w_b['max']\n",
    "\n",
    "    w_b = w_b['weight3'].tolist()\n",
    "    w_b = [(1-split)*w/sum(w_b) for w in w_b]\n",
    "    \n",
    "    \n",
    "    loc_a = [ train.columns.tolist().index(ttt)  for ttt in train_trace.columns]\n",
    "    loc_b = [ train.columns.tolist().index(ttt)  for ttt in train_attr.columns]\n",
    "    customized_weights = np.repeat(0.0, len(train.columns))\n",
    "    customized_weights[loc_a] = w_a\n",
    "    customized_weights[loc_b] = w_b\n",
    "\n",
    "\n",
    "    train = train.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    test = test.apply(lambda x: x*customized_weights, axis= 1)\n",
    "    \n",
    "    encoded_train = train.values.tolist()\n",
    "    encoded_test = test.values.tolist()\n",
    "\n",
    "    encoded_test_attr = test_attr.values.tolist()\n",
    "    for d in metric:\n",
    "        time_start_align = time.time()\n",
    "        \n",
    "        \n",
    "        dists, idxs = nearest_neighbors(np.array(encoded_test), \n",
    "                                    np.array(encoded_train), \n",
    "                                    nbr_neighbors = size, \n",
    "                                    algorithm = 'auto',\n",
    "                                    metric = d,\n",
    "                                    metric_params = None) # 20 , len(sn_encoded_train) \n",
    "        \n",
    "        predict = list()\n",
    "        dist_mean = list()\n",
    "        dist_std = list()\n",
    "        dist_min = list()\n",
    "        dist_max = list()\n",
    "        sim = list()\n",
    "        for i in range(0, len(test_trace)):        \n",
    "            predict.append(  train.index[idxs[i]] )\n",
    "            dist_mean.append(  np.mean(dists[i]) )\n",
    "            dist_std.append(  np.std(dists[i]) )\n",
    "            dist_min.append(  np.min(dists[i]) )\n",
    "            dist_max.append(  np.max(dists[i]) )\n",
    "            sim.append( np.mean( 1/(1+dists[i])) )\n",
    "\n",
    "        time_finish_align = time.time()\n",
    "\n",
    "        with open(\"label_\"+name , \"rb\") as fp:\n",
    "            ref_predict = pickle.load(fp, encoding='utf-8') \n",
    "            \n",
    "        recall_sum = 0\n",
    "        precision_sum = 0\n",
    "        acc_sum1 = 0 \n",
    "        acc_sum2 = 0 \n",
    "        dist1 = 0\n",
    "        dist2 = 0\n",
    "        sim1_sum = 0\n",
    "        sim2_sum = 0\n",
    "        for l in range(0,len(test)):  # change\n",
    "            i1 = ref_predict[l]\n",
    "            acc_1 = sum([1 for j in i1 if j[0] in predict[l]])/len(i1)\n",
    "            \n",
    "            opt1 = train.loc[train.index.isin(i1)]\n",
    "            \n",
    "            apa = train.loc[train.index.isin([rp1 for rp1 in predict[l]])]\n",
    "            \n",
    "            sim1 = 0\n",
    "            \n",
    "            for apa_i in range(len(apa)):\n",
    "                sim1_i = sum( [1-distance.euclidean(apa.iloc[apa_i] , opt1.iloc[opt1_i]) for opt1_i in range(len(opt1)) ] )/len(opt1)                \n",
    "                sim1 = sim1+ sim1_i            \n",
    "            \n",
    "            acc_sum1 = acc_sum1 + acc_1\n",
    "            \n",
    "            sim1_sum = sim1_sum + sim1/(len(apa)*len(opt1))\n",
    "        acc1 = acc_sum1/len(test)\n",
    "        acc2 = acc_sum2/len(test)\n",
    "        \n",
    "        sim1_out = sim1_sum/len(test)\n",
    "        print(sim1_out)\n",
    "        result.loc[len(result)+1] = [name, e, d, acc1, np.mean(dist_mean), np.mean(dist_std),\n",
    "                                np.mean(dist_min), np.mean(dist_max), np.mean(sim1_out),\n",
    "                                (time_finish_align - time_start_align) ]\n",
    "result.to_csv(dir_home+\"/result/result_dist5_\" + name + \"_\" + str(size) +\"_\"  + str(split) + \".csv\", index= False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b13726099ff4a9270d97cd5a303046c40236cea9d4b3d3acf7f22861afad882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
